# models imported from HF TODO@DR - get other models, likely image
model:
    type: notgpt2 #gpt2 
    n_embd: 256 #these dims are for the gpt2 
    n_layer: 12
    n_head: 8
    n_dims: 8 #these should be the d in subspace dimension though I'll probably do different datagen
    n_positions: 500 #TODO@DR prep for position embeddings these would be the context+target like in ADPM

data:
    datagen_case: 0   #0 or 1 for linear or manifold (DR: which was 2 for them; I am skipping mixture)
    datagen_kwargs: {} #(default: settings.py) dict of kwargs for data generation
    datagen_seed: 0 #or None or 15 or 4

DATAGEN_GLOBALS:
    linear:
        sigma2_corruption: 1.0                    # applies to all cases; default 0.5
        style_corruption_orthog: False            # if True, the noise is only in orthogonal directions
        style_origin_subspace: True               # if True, the subspace must contain origin (not affine)
        style_subspace_dimensions: 8         # int or 'random'
        # parameters specific to the Linear case
        sigma2_pure_context: 2.0                  # controls radius of ball of pure of samples (default: 2.0)
        corr_scaling_matrix: None                 # x = Pz; y = A x  \tilde x = A (x + z); (normally identity I_n)

    manifold:
        sigma2_corruption: 0.5                # applies to all cases
        style_corruption_orthog: False        # mandatory for this case
        style_origin_subspace: True           # mandatory for this case
        style_subspace_dimensions: random   # int or 'random'
        # parameters specific to the Manifold case
        radius_sphere: 1.0                  # controls radius of sphere (d-dim manifold S in R^n)
    
    drift_noise_sigma_train_test_linear:  # DR: the only difference from the linear case0 is the drift in added noise std
        sigma2_corruption_train: 0.1                  
        sigma2_corruption_test: 2.0   
        style_corruption_orthog: False           
        style_origin_subspace: True               
        style_subspace_dimensions: 8        
        sigma2_pure_context: 2.0                  
        corr_scaling_matrix: None                 



training:
    task: TBD
    batch_size: 80
    learning_rate: 0.01
    period_save_weights: 4
    train_steps:
    resume_id:
    epochs: 4
    optimizer_choice: adamw # DR: only one choice bc if no wd basically adam
    wd: 0.0 # weight decay in adamw
    scheduler_kwargs:
        choice: cosine # cosine or None
        warmup: 1 # num warmup steps for cosine scheduler
    restart_nn_instance: #instance of a sequence model class (e.g. TransformerModelV1)
    restart_dataset: #(None) or 6-tuple (X_train, y_train, X_test, y_test, train_dict, test_dict) for dataset
    nn_model: TransformerModelV1noresOmitLast #string for a sequence model class (e.g. 'TransformerModelV1')
    context_len: 500 #(default: 500) int, context length of input token sequence (X.shape[-1])
    dim_n: 32 #(default: 32) int, ambient dimension
    seed_torch: #(default: None) None or an int (sets net weights; possibly batch order/shuffle)
    train_plus_test_size: 1000 # settings for dataset construction + training (from their settings file)
    num_W_in_dataset: 1000
    context_examples_per_W: 1
    samples_per_context_example: 1
    full_loss_sample_interval: 4
    flag_save_dataset: False
    flag_vis_loss: False
    flag_vis_weights: False 
    flag_vis_batch_perf: False
    flag_vis_grad: False
    test_ratio: 0.2
    nwork: 0
    skip_PCA_heuristic_slow: True



nlmeans:
    nlm_k: 8
    nlm_h: 0.1
    nlm_patch_size: 4
    nlm_search_size: 8
    nlm_stride: 4

# settings for visualization paper-like
vis:
    COLOR_TARGET: '#97C6CF'
    COLOR_PRED: '#89C09F'
    COLOR_INPUT: '#E9B24C'
