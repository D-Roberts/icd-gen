# models imported from HF TODO@DR - get other models, likely image
model:
    type: notgpt2 #gpt2 
    n_embd: 256 #these dims are for the gpt2 
    n_layer: 12
    n_head: 8
    n_dims: 8  
    n_positions: 500 #TODO@DR prep for position embeddings these would be the context+target like in ADPM


DATAGEN_GLOBALS:
    linear:
        sigma2_corruption: 1.0                    # applies to all cases; default 0.5
        style_corruption_orthog: False            # if True, the noise is only in orthogonal directions
        style_origin_subspace: True               # if True, the subspace must contain origin (not affine)
        style_subspace_dimensions: 8         # int or 'random'
        # parameters specific to the Linear case
        sigma2_pure_context: 2.0                  # controls radius of ball of pure of samples (default: 2.0)
        corr_scaling_matrix: None                 # x = Pz; y = A x  \tilde x = A (x + z); (normally identity I_n)
        
    
    drift_noise_sigma_train_test_linear:  
    # DR: the only difference from the linear case0 is the shift in added noise 
    # std from train to test
        sigma2_corruption_train: 0.1                  
        sigma2_corruption_test: 2.0   
        style_corruption_orthog: False           
        style_origin_subspace: True               
        style_subspace_dimensions: 8        
        sigma2_pure_context: 2.0                  
        corr_scaling_matrix: None        

        
# new data generation to exploit vit patch association learning in context
# and gamma noise
   
    
grouped_patches:
    datagen:
        D: 10 # num patches; this would be seq len
        L: 2 # number of groups of patches in partition; so then group/set dim is C=D/L
        q: 0.5 # now q is log(d)/D in make data to control SNR but might consider a param
        #for it
        N: 5000 # train dataset size
        d: 100 # patch size 10X10

    gamma:
        galpha: [2.26, 3.62] # per group of patches gamma noise distrib param
        gbeta: [1.5, 0.8]  # per group of patchesl these could be sampled in ablation
        # both params > 0
        # gamma noise is multiplicative and > 0; 


training:
    batch_size: 256
    learning_rate: 0.01

    period_save_weights: 4
    train_steps:

    loss: MSE # arg not used TODO@DR; MAE, SURE and Jens_Shan should be options

    epochs: 5
    optimizer_choice: adamw 
    wd: 0.0 # weight decay in adamw; add some since vit likes either this or sgd with mom
    scheduler_kwargs:
        choice: cosine # cosine or None; works best
        warmup: 0 # num warmup steps for cosine scheduler
    nn_model: TransformerModelV2 #one attn layer with softmax
    
    
    seed_torch: #(default: None) None or an int (sets net weights; possibly batch order/shuffle)
    train_plus_test_size: 5000 # settings for dataset construction + training (from their settings file)
    full_loss_sample_interval: 4
    flag_save_dataset: False
    flag_vis_loss: False
    flag_vis_weights: False 
    flag_vis_batch_perf: False
    flag_vis_grad: False
    test_ratio: 0.2
    nwork: 0

    context_len: 10 #(default: 500) int, context length of input token sequence (X.shape[-1])
    dim_n: 200 #(default: 32) int, ambient dimension; for fused patches is 200 now

    linear: #for icml'25 icl denoise paper style
        num_W_in_dataset: 1000
        context_examples_per_W: 1
        samples_per_context_example: 1
        skip_PCA_heuristic_slow: True
        
       
