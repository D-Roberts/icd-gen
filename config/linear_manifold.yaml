# models imported from HF
model:
    family: 1softmaxT1H #gpt2
    n_embd: 256 #these dims are for the gpt2 TODO @DR make all archi choices params here
    n_layer: 12
    n_head: 8
    n_dims: 5
    n_positions: 11

data:
    datagen_case: 0   #0 or 1 for linear or manifold (DR: which was 2 for them; I am skipping mixture)
    datagen_kwargs: {} #(default: settings.py) dict of kwargs for data generation
    datagen_seed: 0 #or None or 15 or 4

DATAGEN_GLOBALS:
    linear:
        sigma2_corruption: 0.8                    # applies to all cases; default 0.5
        style_corruption_orthog: False            # if True, the noise is only in orthogonal directions
        style_origin_subspace: True               # if True, the subspace must contain origin (not affine)
        style_subspace_dimensions: random         # int or 'random'
        # parameters specific to the Linear case
        sigma2_pure_context: 2.0                  # controls radius of ball of pure of samples (default: 2.0)
        corr_scaling_matrix: None                 # x = Pz; y = A x  \tilde x = A (x + z); (normally identity I_n)

    manifold:
        sigma2_corruption: 0.5                # applies to all cases
        style_corruption_orthog: False        # mandatory for this case
        style_origin_subspace: True           # mandatory for this case
        style_subspace_dimensions: random   # int or 'random'
        # parameters specific to the Manifold case
        radius_sphere: 1.0                  # controls radius of sphere (d-dim manifold S in R^n)
    drift_noise_sigma_train_test:


training:
    task: TBD
    batch_size: 4
    learning_rate: 0.01
    period_save_weights: 1
    train_steps:
    resume_id:
    epochs: 3
    optimizer_choice: adamw # DR: only one choice bc if no wd basically adam
    wd: 0.0 # weight decay in adamw
    scheduler_kwargs:
        choice: # cosine or None
        warmup: 1 # num warmup steps for cosine scheduler
    restart_nn_instance: #instance of a sequence model class (e.g. TransformerModelV1)
    restart_dataset: #(None) or 6-tuple (X_train, y_train, X_test, y_test, train_dict, test_dict) for dataset
    nn_model: TransformerModelV1noresOmitLast #string for a sequence model class (e.g. 'TransformerModelV1')
    context_len: 8 #(default: 500) int, context length of input token sequence (X.shape[-1])
    dim_n: 16 #(default: 32) int, ambient dimension
    seed_torch: #(default: None) None or an int (sets net weights; possibly batch order/shuffle)
    train_plus_test_size: 40 # settings for dataset construction + training (from their settings file)
    num_W_in_dataset: 40
    context_examples_per_W: 1
    samples_per_context_example: 1
    full_loss_sample_interval: 4
    flag_save_dataset: False
    flag_vis_loss: False
    flag_vis_weights: False #DR: they get logged in Comet image tab
    flag_vis_batch_perf: False
    test_ratio: 0.1
    nwork: 0
    skip_PCA_heuristic_slow: True

# settings for visualization paper-like
vis:
    COLOR_TARGET: '#97C6CF'
    COLOR_PRED: '#89C09F'
    COLOR_INPUT: '#E9B24C'
