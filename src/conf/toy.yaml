#yeap this is training; I think using mps as well. loss is going down. learn 1 dim with 1 layer 1 head 11 pos
model:
    family: gpt2
    n_embd: 256
    n_layer: 12
    n_head: 8
    n_dims: 5
    n_positions: 11

training:
    task: linear_regression
    data: gaussian
    num_training_examples:
    task_kwargs: {}
    batch_size: 80
    learning_rate: 0.0001
    save_every_steps: 1000
    keep_every_steps: 100000
    num_tasks:
    train_steps: 3 #5001
    resume_id:
    curriculum: #recall these are duplicates
        dims:
            start: 1
            end: 1
            inc: 1
            interval: 2000
        points:
            start: 11
            end: 11
            inc: 2
            interval: 2000


out_dir: ../models/linear_regression

wandb:
    name: "linear_regression_toy"
    project: in-context-training
    entity: denisa-roberts
    notes:
    log_every_steps: 100

