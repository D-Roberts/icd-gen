# models imported from HF
model:
    family: 1softmaxT1H #gpt2 
    n_embd: 256
    n_layer: 12
    n_head: 8
    n_dims: 5
    n_positions: 11

data:
    datagen_case: 0   #0 or 1 for linear or manifold (DR: which was 2 for them; I am skipping mixture)
    datagen_kwargs: {} #(default: settings.py) dict of kwargs for data generation
    datagen_seed: 0 #or None or 15 or 4

DATAGEN_GLOBALS:
    linear: 
        sigma2_corruption: 0.8                    # applies to all cases; default 0.5
        style_corruption_orthog: False            # if True, the noise is only in orthogonal directions
        style_origin_subspace: True               # if True, the subspace must contain origin (not affine)
        style_subspace_dimensions: random         # int or 'random'
        # parameters specific to the Linear case
        sigma2_pure_context: 2.0                  # controls radius of ball of pure of samples (default: 2.0)
        corr_scaling_matrix: None                 # x = Pz; y = A x  \tilde x = A (x + z); (normally identity I_n)

    manifold: 
        sigma2_corruption: 0.5                # applies to all cases
        style_corruption_orthog: False        # mandatory for this case
        style_origin_subspace: True           # mandatory for this case
        style_subspace_dimensions: random   # int or 'random'
        # parameters specific to the Manifold case
        radius_sphere: 1.0                  # controls radius of sphere (d-dim manifold S in R^n)


training:
    task: in-context denoise
    batch_size: 80
    learning_rate: 0.01
    period_save_weights: 1
    train_steps:
    resume_id:
    epochs: 100
    optimizer_choice: adam
    scheduler_kwargs: 
        choice: #none, multistep or cosine
        milestones: [5, 10] #[30, 80]
        gamma: 0.1
    restart_nn_instance: #instance of a sequence model class (e.g. TransformerModelV1)
    restart_dataset: #(None) or 6-tuple (X_train, y_train, X_test, y_test, train_dict, test_dict) for dataset
    nn_model: TransformerModelV1noresOmitLast #string for a sequence model class (e.g. 'TransformerModelV1')
    context_len: 20 #(default: 500) int, context length of input token sequence (X.shape[-1])
    dim_n: 32 #(default: 32) int, ambient dimension
    seed_torch: #(default: None) None or an int (sets net weights; possibly batch order/shuffle)
    train_plus_test_size: 1000 # settings for dataset construction + training (from their settings file)
    num_W_in_dataset: 1000
    context_examples_per_W: 1
    samples_per_context_example: 1
    full_loss_sample_interval: 4
    flag_save_dataset: False #they aren't that big
    flag_vis_loss: True 
    flag_vis_weights: True 
    flag_vis_batch_perf: True 
    test_ratio: 0.2
    nwork: 0
    skip_PCA_heuristic_slow: False

wandb:
    name: "linear_case"
    project: in-context-denoising
    entity: denisa-roberts
    notes:
    log_every_steps: 1

# settings for visualization
vis:
    COLOR_TARGET: '#97C6CF'
    COLOR_PRED: '#89C09F'
    COLOR_INPUT: '#E9B24C'